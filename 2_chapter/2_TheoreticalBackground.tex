\chapter{Theoretical Background}
\label{chap:theory}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Medical Background}
\label{sec:medical_background}
In this chapter, the focus is on the theoretical background of the medical topics covered in the scope of this work, which is the basis for the development of the processing pipeline. Firstly, AC will be introduced, followed by a description of OA and its common treatment methods. The chapter ends with the depiction of state-of-the-art methods for clinically treating OA.\cite{Yoo2024}
\subsection{Articular Cartilage}
\subsection{Osteoarthritis and its Treatment}
\subsection{3D Bioprinting Techniques}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Dense 3D Reconstruction}
\label{sec:dense_3d_theory}
This chapter presents the theoretical background relevant to this work, with a focus on dense 3D reconstruction from monocular sequences.
First, related work is reviewed, covering the evolution from traditional geometry-based methods such as Structure-from-Motion (SfM) and Multi-View Stereo (MVS) to recent feed-forward transformer-based reconstruction approaches.
Then, the reconstruction model adopted in this work, Depth Anything 3 (DA3), is introduced in detail, providing the foundation for the subsequent methodology.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Related Work}
\subsubsection*{Traditional Geometry-based 3D Reconstruction}
\label{sec:classical_reconstruction}
The field of 3D reconstruction has been traditionally dominated by pipeline-based approaches, namely Structure-from-Motion (SfM)\cite{schonberger2016SfM} and Multi-View Stereo (MVS)\cite{schonberger2016MVS}.

% Structure from Motion
\textbf{Structure from Motion} is a classic computer vision problem [45, 77, 80] that involves estimating camera parameters
and reconstructing sparse point clouds from a set of images
of a static scene captured from different viewpoints. The
traditional SfM pipeline [2, 36, 70, 94, 103, 134] consists
of multiple stages, including image matching, triangulation,
and bundle adjustment. COLMAP [94] is the most popular framework based on the traditional pipeline.\cite{wang2025vggt}

Figure\ref{fig:classical_reconstruction_SfM} illustrates a fundamental SfM framework.The pipeline starts from multi-view image inputs, establishes inter-image correspondences through feature matching and geometric verification, and then incrementally estimates camera poses and 3D scene structure. The reconstruction is continuously refined through triangulation and bundle adjustment, resulting in a globally consistent 3D representation.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{4_images/2_TheoreticalBackground/classical_reconstruction_SfM.png}
    \caption{Structure-from-Motion pipeline}
    \label{fig:classical_reconstruction_SfM}
\end{figure}


% Multi-view Stereo
\textbf{Multi-view Stereo (MVS)} aims to densely reconstruct the geometry of a scene from multiple overlapping images, typically assuming known camera parameters which are often estimated via SfM~\cite{wang2025vggt}.
The fundamental principle underlying MVS is the Photo Consistency Assumption, which posits that a 3D point in space should exhibit a similar appearance (color or intensity) when projected onto different image planes, provided the surface is approximately Lambertian. To efficiently identify corresponding points across views, MVS leverages Epipolar Geometry. Instead of searching the entire image domain, pairwise matching is constrained to epipolar lines determined by the relative camera poses, thereby significantly reducing computational complexity. Based on these geometric constraints, the algorithm estimates a depth map for each reference image by aggregating information from neighboring views. Finally, in the Fusion stage, these individual depth maps are integrated into a globally consistent point cloud or mesh, filtering out outliers to produce a high-fidelity dense estimation of the scene geometry.

% early learning methods in SfM/MVS
Early learning methods injected robustness at the component
level: learned detectors [20], descriptors for matching [22], and differentiable optimization layers that expose
pose/depth updates to gradient flow [31, 33, 62]. 
On the dense side, cost-volume networks [106, 114] for MVS replaced hand-crafted regularization with 3D CNNs, improving depth accuracy especially at large baselines and thin structures compared with classical PatchMatch.\cite{lin2025da3}
%% problem with early learning methods 
They did reduce engineering complexity and demonstrated the feasibility of learned joint depth pose estimation, but they often struggled with scalability, generalization, and handling arbitrary input cardinalities.\cite{lin2025da3}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{Feed-forward Transformer-based 3D Reconstruction}
\label{sec:feed_forward_reconstruction}
% Intro
%% problem with traditional methods
Traditional Geometry-based methods remain strong on well-textured scenes, but their modularity and brittle correspondences complicate robustness under low texture, specularities, or large viewpoint changes.

The emergence of feed-forward 3D reconstruction methods is closely tied to the introduction of attention mechanisms. Unlike convolutional operations that rely on local receptive fields, attention enables explicit modeling of long-range dependencies by computing pairwise interactions between elements in a sequence.
In the context of multi-view geometry, this property is particularly appealing, as 3D reconstruction inherently requires reasoning over global spatial relationships and correspondences across views. By allowing features from different images to directly attend to each other, attention mechanisms provide a natural alternative to explicit feature matching and geometric verification used in traditional pipelines.

Leveraging attention-based architectures, recent approaches formulate 3D reconstruction as a feed-forward prediction problem, where geometric quantities such as depth, camera motion, or point maps are directly inferred from image features. This paradigm eliminates the need for iterative optimization and decoupled processing stages, enabling end-to-end learning of visual geometry from data.

\subsubsection*{DUSt3R: A Pioneering Feed-forward Transformer-based Method}
A turning point came with DUSt3R,a radically novel paradigm for Dense and Unconstrained Stereo 3D Reconstruction of arbitrary image\cite{2024dust3r}, which leveraged transformers to directly predict point map between two views and compute both depth and relative pose in a purely feed-forward manner. This work laid the foundation for subsequent transformer-based methods aiming to unify multi-view geometry estimation at scale.\cite{lin2025da3}

As illustrated in Fig.~\ref{fig:dust3r_framework}, given two input images $I_1$ and $I_2$, both images are first processed by a shared image feature extractor to obtain dense visual representations. The feature extractor can be implemented using either convolutional neural networks or transformer-based architectures; in DUSt3R, a Vision Transformer (ViT) is adopted as the encoder, whose architectural details will be discussed in the next chapter.
The extracted feature tokens from the two views are then passed to two transformer decoders, which iteratively exchange information through cross-attention. This mechanism allows features from one view to attend to and reason about features from the other view, enabling implicit correspondence discovery without explicit feature matching.

Based on the fused multi-view features, two regression heads predict dense point maps for each input image, along with associated confidence maps. Importantly, both point maps are expressed in a common coordinate frame defined by the first camera view $I_1$, thereby establishing a shared geometric reference across the two images. From the predicted point maps, camera intrinsics such as focal lengths can be estimated using the Weiszfeld algorithm \cite{weiszfeld1937point}, and relative camera poses can be recovered using minimal solvers such as 3-point RANSAC \cite{nister2004fivepointmethod,matas2005ransac}.

By directly regressing dense geometry in a feed-forward manner, DUSt3R eliminates the need for explicit feature matching, geometric verification, and iterative optimization, demonstrating the feasibility of learning-based geometric reasoning from image pairs. \cite{maggio2025vggtslam}
\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{4_images/2_TheoreticalBackground/feed_forward_reconstruction_dust3r.png}
    \caption{DUSt3R framework overview}
    \label{fig:dust3r_framework}
\end{figure}


\subsubsection*{Follow-up Works}
MASt3R follows a similar design but also outputs descriptors that can be used to generate pairwise correspondences between the two frames. MASt3R-SFM [14] demonstrates global optimization of multiple images using MASt3R but computation scales quickly with the number of frames. \cite{maggio2025vggtslam}

To extend the idea of DUSt3R to multiple frames, Spann3R [67] leverages a learned memory module and Cut3R [69] uses a recurrent state model. Both can incrementally reconstruct a scene using multiple images, but are each limited to short sequences. Recently, Pow3R [27] extends the DUSt3R framework to optionally take in any estimates of any combination of camera intrinsics, poses, and depth (which may be sparse or dense) and demonstrates substantial improvement in scene reconstruction and pose estimation given the added inputs. Splatt3R [60] extends the DUSt3R idea to Gaussian Splatting [29] by directly outputting the Gaussian Splatting parameters given two views, and PreF3R [8] extends this to multiple views using a similar memory framework as Spann3R. Reloc3r [13] modifies the DUSt3R framework for directly outputs relative camera poses and uses motion averaging to recover absolute poses with respect to a map database.\cite{maggio2025vggtslam}

\subsubsection*{Visual Geometry Grounded Transformer}
While DUSt3R and its follow-up works demonstrate that feed-forward models can successfully infer dense geometry from image pairs, their pairwise formulation inherently limits scalability and global reasoning. Extending such methods to multi-view settings typically requires additional post-processing steps, such as pose graph optimization or incremental fusion, which reintroduce elements of traditional pipelines.

These limitations motivate the need for a unified framework that can jointly reason over an arbitrary number of views and directly model global geometric relationships in a feed-forward manner.

As a culmination of feed-forward reconstruction models inspired by DUSt3R, VGGT (Visual Geometry Grounded Transformer) extends pairwise geometric reasoning to arbitrary-length image sequences within a unified architecture. An overview of the VGGT framework is shown in Figure~\ref{fig:vggt_framework}. Given a set of input images, VGGT first decomposes each image into a sequence of visual tokens using a pretrained image feature extractor. In the original formulation, self-supervised features obtained from DINO are employed to provide semantically rich and geometrically consistent representations. These per-image token sequences are then concatenated, and camera-specific tokens are appended to enable explicit modeling of camera parameters.

The core of VGGT consists of multiple transformer layers that alternate between global attention and frame-wise attention. Global attention allows tokens from different views to directly interact, facilitating cross-view information exchange and long-range geometric reasoning. In contrast, frame-wise attention focuses on refining features within each individual image. By alternating between these two attention mechanisms, VGGT jointly captures both intra-view structure and inter-view geometric relationships across the entire image set.

Based on the aggregated token representations, VGGT employs multiple prediction heads to regress diverse geometric quantities. A dedicated camera head estimates camera intrinsics and extrinsics, while dense prediction transformer (DPT) heads are used to generate dense depth maps, point maps, and feature tracks. Importantly, these outputs are expressed in a common coordinate frame, enabling consistent multi-view reconstruction without explicit correspondence estimation or incremental optimization.

By unifying multi-view feature aggregation and geometry prediction within a single feed-forward transformer, VGGT represents a significant step toward holistic and scalable dense 3D reconstruction from unconstrained image collections.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{4_images/2_TheoreticalBackground/feed_forward_reconstruction_vggt.png}
    \caption{VGGT framework overview}
    \label{fig:vggt_framework}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{State of the Art: DA3}
\label{sec:da3_reconstruction}
% 肯定 VGGT 的贡献
While VGGT represents a significant advancement in unified feed-forward multi-view reconstruction, it still leaves room for improvement in terms of efficiency, scalability, and generalization. 
% 明确指出三点问题：架构庞大（~1B 参数） 任意长度输入 → 显存 / 算力开销大 不适合轻量化部署
The VGGT framework is characterized by a large and complex architecture, with model sizes reaching up to one billion parameters, which makes it unsuitable for lightweight deployment scenarios. Although VGGT supports arbitrary-length input sequences, this flexibility comes at the cost of substantial memory consumption and computational overhead, especially when processing long image sequences.
% 强调泛化问题 在“完全未见过的场景”（如内窥镜）仍可能失败
Moreover, despite being trained on large-scale and diverse datasets covering a wide range of common visual scenes, VGGT may still fail when confronted with domains that differ significantly from its training distribution, such as endoscopic imagery. These limitations highlight the need for a more compact and robust reconstruction model that preserves the strengths of VGGT while addressing its practical shortcomings.
% DA3 正式登场
To this end, Depth Anything 3 (DA3) is introduced as an improved variant built upon the VGGT paradigm. Building upon this insight, DA3 proposes a unified feed-forward model that reconstructs the visual space from arbitrary visual inputs, including multi-view image collections and video streams. Given any number of input views, with or without known camera poses, DA3 jointly predicts dense depth maps and ray maps that can be directly fused into geometrically consistent 3D point clouds.

Compared to VGGT, DA3 significantly simplifies the overall framework by relying on a single DINO-based transformer backbone and enabling cross-view reasoning through token rearrangement rather than explicit multi-branch architectures. This design reduces the model size to a configurable range between 0.11B and 0.35B parameters, making DA3 substantially more suitable for lightweight and resource-constrained deployment while maintaining strong reconstruction performance.

In addition, DA3 leverages a powerful depth teacher model during training, allowing it to learn robust geometric priors from large-scale data. This training strategy improves generalization and enables the model to remain effective even in previously unseen domains. Finally, DA3 introduces a depth-ray representation as a minimal yet sufficient geometric abstraction, which explicitly encodes both scene structure and camera motion while avoiding redundant or constrained prediction targets.

The following sections provide a detailed description of the DA3  depth-ray formulation, its architecture, and the associated training paradigm.

The following sections provide a detailed description of the DA3  depth-ray formulation, its architecture, and the associated training paradigm.

% Depth-ray representation
\subsubsection*{Depth-ray Representation}
At the core of DA3 lies the depth--ray representation, which explicitly encodes both scene geometry and camera motion without directly predicting constrained rotation matrices. For each pixel $p=(u,v,1)^\top$, the model predicts a depth value $D(u,v)$ together with a corresponding ray
\begin{center}
    $ r=(t,d), \quad d=RK^{-1}p, $
\end{center}
where $t \in \mathbb{R}^3$ denotes the camera center and $d \in \mathbb{R}^3$ is the unnormalized ray direction in world coordinates.

The predicted ray map is denoted as
\begin{center}
    $ M \in \mathbb{R}^{H \times W \times 6}, $
\end{center}
where the first three channels $M(:,:,:3)$ store per-pixel ray origins and the last three channels $M(:,:,3:)$ store the corresponding ray directions. Given the depth and ray predictions, a 3D point can be recovered by
\begin{center}
    $ P(u,v) = t + D(u,v) \cdot d. $
\end{center}

Beyond 3D point reconstruction, the depth--ray representation also enables direct recovery of camera parameters. The camera center $t_c$ is estimated by averaging the per-pixel ray origins:
\begin{center}
    $ t_c = \frac{1}{HW} \sum_{h=1}^{H} \sum_{w=1}^{W} M(h,w,:3). $
\end{center}

To recover the camera rotation $R$ and intrinsic matrix $K$, DA3 formulates the problem as estimating a homography. An identity canonical camera is first defined with intrinsics $K_I=I$. For a pixel $p$, the corresponding ray direction in this canonical camera is
\begin{center}
    $ d_I = K_I^{-1} p = p. $
\end{center}

The transformation from the canonical ray $d_I$ to the ray direction $d_{\text{cam}}$ in the target camera coordinate system can be written as
\begin{center}
    $ d_{\text{cam}} = KR d_I, $
\end{center}
which establishes a homography relationship
\begin{center}
    $ H = KR. $
\end{center}

The homography $H$ is then estimated by minimizing the geometric error between transformed canonical rays and the predicted ray directions:
\begin{center}
    $ H^* = \arg \min_{\|H\|=1} \sum_{h=1}^{H} \sum_{w=1}^{W} \| H p_{h,w} \times M(h,w,3:) \|. $
\end{center}

This optimization corresponds to a standard least-squares problem and can be efficiently solved using the Direct Linear Transform (DLT) algorithm. Once the optimal homography $H^*$ is obtained, the camera intrinsics $K$ and rotation $R$ are uniquely recovered via RQ decomposition, exploiting the upper-triangular structure of $K$ and the orthonormality of $R$.

By jointly modeling depth and rays, this representation avoids explicit pose constraints while ensuring geometric consistency across views, and provides a compact yet expressive abstraction for feed-forward 3D reconstruction.

% Architecture 
\subsubsection*{Architecture}
\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{4_images/2_TheoreticalBackground/da3_reconstruction_da3.png}
    \caption{DA3 framework overview}
    \label{fig:da3_framework}
\end{figure}
As illustrated in Fig.~\ref{fig:da3_framework} shows an overview of the DA3 framework.
Architecturally, DA3 adopts a deliberately simple design centered around a single pretrained Vision Transformer (ViT) backbone. For an input set 
\begin{center}
    $\mathcal{I}=\{I_i\}_{i=1}^{N_v}, I_i \in \mathbb{R}^{H \times W \times 3}$
\end{center}
the transformer serves as a shared feature extractor across all views, enabling the model to naturally handle variable input cardinalities.

Cross-view geometric reasoning is achieved through an input-adaptive self-attention mechanism. Instead of introducing additional transformer stacks or specialized modules, DA3 rearranges token sequences in selected layers, allowing standard self-attention to operate either within individual views or across all views jointly.

Dense geometric predictions in DA3 are produced by a novel dual Dense Prediction Transformer (Dual-DPT) head, which jointly regresses dense depth maps and ray maps from a shared set of visual features. As illustrated in Fig.~\ref{fig:da3_architecture}, the Dual-DPT head first processes backbone features through a common set of reassembly modules, ensuring that both prediction tasks operate on a unified and compact intermediate representation.

After this shared processing stage, the features are routed into two parallel branches, corresponding to depth prediction and ray prediction, respectively. Each branch applies its own set of fusion layers to aggregate contextual information relevant to the target geometric quantity. Finally, separate output layers generate the depth map and ray map predictions. By sharing the majority of the feature processing pipeline and diverging only at the final fusion stage, the Dual-DPT design promotes strong interaction between depth and ray estimation while avoiding redundant intermediate representations.

In addition to dense geometric outputs, DA3 optionally predicts camera parameters through a lightweight camera head operating on dedicated camera tokens, incurring minimal computational overhead. This joint prediction framework further reinforces geometric consistency without compromising the overall efficiency of the model.
\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{4_images/2_TheoreticalBackground/da3_reconstruction_dpt.png}
    \caption{DA3 Dual-DPT Head}
    \label{fig:da3_architecture}
\end{figure}

% Training
\subsubsection*{Training}
To train a generalist geometry model across diverse data sources, DA3 adopts a teacher--student learning paradigm. A teacher model is first trained on large-scale synthetic data to generate dense and reliable pseudo-depth supervision, which is then combined with sparse or noisy real-world depth measurements. This strategy enables effective knowledge transfer while preserving geometric fidelity across diverse domains.

Formally, the DA3 model $F_\theta$ maps an input image set $\mathcal{I}$ to a set of geometric predictions, including a depth map $\hat{D}$, a ray map $\hat{R}$, and an optional camera pose $\hat{c}$:
\begin{center}
    $F_\theta(\mathcal{I}) \to \{\hat{D}, \hat{R}, \hat{c}\}.$
\end{center}
The camera pose prediction is optional and primarily included for practical convenience.

Prior to loss computation, all ground-truth geometric signals are normalized by a common scale factor to ensure consistent magnitude across different modalities. This scale is defined as the mean $\ell_2$ norm of the valid reprojected 3D point maps $P$, which stabilizes optimization when jointly supervising depth, rays, and reconstructed points.

The overall training objective is defined as a weighted sum of several loss terms:
\begin{center}
    $L = L_D(\hat{D}, D) + L_M(\hat{R}, M) + L_P(\hat{D} \odot d + t, P) + \beta L_C(\hat{c}, c) + \alpha L_{\text{grad}}(\hat{D}, D),$
\end{center}
where $L_D$ supervises depth prediction, $L_M$ enforces consistency of ray maps, $L_P$ penalizes geometric errors in reconstructed 3D points, and $L_C$ is an optional camera pose loss.

The depth supervision term is defined as
\begin{center}
    $ L_D(\hat{D}, D; D_c) = \frac{1}{|\Omega|} \sum_{p \in \Omega} m_p (D_{c,p} |\hat{D}_p - D_p| - \lambda_c \log D_{c,p}), $
\end{center}
where $D_{c,p}$ denotes the confidence associated with the ground-truth depth $D_p$, $m_p$ is a validity mask, and $\Omega$ denotes the set of valid pixels. All loss terms are based on the $\ell_1$ norm.

To further regularize the depth predictions, a gradient loss is introduced:
\begin{center}
    $ L_{\text{grad}}(\hat{D}, D) = \| \nabla_x \hat{D} - \nabla_x D \|_1 + \| \nabla_y \hat{D} - \nabla_y D \|_1, $
\end{center}
where $\nabla_x$ and $\nabla_y$ denote horizontal and vertical finite difference operators. This term preserves sharp depth discontinuities while encouraging smoothness in planar regions. In practice, the weighting factors are set to $\alpha=1$ and $\beta=1$.

This training strategy enables DA3 to effectively leverage heterogeneous datasets while maintaining consistent geometric supervision, resulting in a scalable and robust feed-forward reconstruction model.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{SLAM and Optimization}
\label{sec:slam_and_optimization}
Visual Simultaneous Localization and Mapping (SLAM) estimates the camera trajectory while building a map of the environment from image streams. A comprehensive survey of the field can be found in~\cite{cadena2016slam}. Classical visual SLAM systems are often keyframe-based and feature-driven, e.g., PTAM~\cite{klein2007ptam} and ORB-SLAM/ORB-SLAM2~\cite{murartal2015orbslam,murartal2017orbslam2}. In contrast, direct methods such as LSD-SLAM~\cite{engel2014lsdslam} and DSO~\cite{engel2018dso} avoid explicit feature matching and minimize photometric error over selected pixels. These two families share a common structure: a front-end responsible for data association and pose tracking, and a back-end that enforces global consistency via optimization.
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{Standard Visual SLAM Pipeline}
Despite differences in sensors and implementations, most classical visual SLAM systems follow a modular pipeline that separates tracking, mapping, and optimization~\cite{cadena2016slam,murartal2015orbslam,murartal2017orbslam2}, as illustrated in Fig.~\ref{fig:slam_pipeline}.

\begin{enumerate}
    \item \textbf{Initialization:} the system is bootstrapped by estimating an initial set of camera poses and map points from a short image sequence, typically using monocular epipolar geometry or direct depth measurements in stereo/RGB-D settings.

    \item \textbf{Tracking (front-end):} for each incoming frame, visual features (e.g., ORB keypoints) are extracted and matched against the local map. The current camera pose is estimated by solving a Perspective-$n$-Point (PnP) or epipolar geometry problem, with robust outlier rejection commonly performed using RANSAC to handle incorrect correspondences.

    \item \textbf{Local mapping:} selected keyframes are inserted into the map, new 3D landmarks are triangulated, and redundant points or keyframes are culled. To maintain local consistency, a local bundle adjustment (BA) is applied to jointly refine recent camera poses and map points.

    \item \textbf{Loop closure and global optimization:} revisited places are detected using visual place recognition. Once a loop is confirmed, global consistency is enforced by pose graph optimization or global bundle adjustment, correcting accumulated drift in camera poses and the map.
\end{enumerate}

This clear separation between a fast front-end for real-time pose tracking and a back-end for global optimization has been a defining characteristic of classical visual SLAM systems.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{4_images/2_TheoreticalBackground/slam_and_optimization_orbslam.png}
    \caption{Standard visual SLAM pipeline}
    \label{fig:slam_pipeline}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{Robust Estimation with RANSAC}
In practical scenes, data association inevitably contains outliers due to repetitive textures, motion blur, and dynamic objects. RANSAC (Random Sample Consensus) addresses this by repeatedly sampling minimal sets to hypothesize a model, scoring it by the number of inliers, and finally re-estimating the model using the best consensus set~\cite{fischler1981ransac}. In visual SLAM, RANSAC is widely used for essential matrix estimation (e.g., the five-point method) and for PnP pose estimation~\cite{nister2004fivepointmethod,matas2005ransac}. A typical iteration evaluates the geometric residual
\begin{center}
    $ \| \mathbf{x}' - \pi(\mathbf{T}\,\mathbf{X}) \| < \tau, $
\end{center}
where $\pi(\cdot)$ denotes the projection model, $\mathbf{T}\in SE(3)$ is the pose, and $\tau$ is an inlier threshold. Given an inlier ratio $w$ and a minimal sample size $s$, the required number of iterations for confidence $p$ is $N=\log(1-p)/\log(1-w^s)$~\cite{fischler1981ransac}. This procedure provides a robust front-end estimate that can be further refined by non-linear optimization.
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{Bundle Adjustment}
Bundle adjustment (BA) is the core optimization that jointly refines camera poses and 3D landmark positions by minimizing reprojection error~\cite{triggs2000bundle}. Given camera poses $\{\mathbf{T}_i\}$ and landmarks $\{\mathbf{X}_j\}$, the classical objective is
\begin{center}
    $ \min_{\{\mathbf{T}_i,\mathbf{X}_j\}} \sum_{i,j} \rho\left(\left\| \mathbf{x}_{ij} - \pi(\mathbf{T}_i\,\mathbf{X}_j) \right\|^2\right), $
\end{center}
where $\mathbf{x}_{ij}$ is the observed feature and $\rho(\cdot)$ is a robust loss. BA is typically solved with Levenberg--Marquardt using sparse linear algebra and Schur complement to eliminate point variables efficiently~\cite{triggs2000bundle,agarwal2010bundle}. In SLAM, local BA is executed on a sliding window of recent keyframes for real-time performance, while pose-graph optimization (e.g., with g2o) is used to enforce global consistency after loop closure~\cite{kummerle2011g2o}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{Pose and Map Transformations on SE(3) and Sim(3)}
Rigid-body poses in SLAM are commonly represented on the Lie group $SE(3)$, where a transformation is
\begin{center}
    $ \mathbf{T} =
    \begin{bmatrix}
    \mathbf{R} & \mathbf{t} \\
    \mathbf{0}^\top & 1
    \end{bmatrix}, \quad \mathbf{R}\in SO(3),\ \mathbf{t}\in\mathbb{R}^3. $
\end{center}
Small updates are applied in the Lie algebra via the exponential map,
$
\mathbf{T} \leftarrow \exp(\hat{\boldsymbol{\xi}})\,\mathbf{T},
$
where $\boldsymbol{\xi}\in\mathbb{R}^6$ is a twist vector~\cite{barfoot2017state,sola2018lie}. This representation enables stable optimization in BA and pose-graph problems.

For monocular SLAM, scale is unobservable and drift accumulates over time. Loop closure often requires a similarity transformation $Sim(3)$ that includes scale:
\begin{center}
    $ \mathbf{S} =
    \begin{bmatrix}
    s\mathbf{R} & \mathbf{t} \\
    \mathbf{0}^\top & 1
    \end{bmatrix}, \quad s>0. $
\end{center}
Closed-form alignment between two point sets under $Sim(3)$ can be obtained by Umeyama's method~\cite{umeyama1991least}, and such transformations are routinely used for loop closure correction in monocular systems like ORB-SLAM~\cite{murartal2015orbslam}. Together, $SE(3)$ and $Sim(3)$ provide the mathematical foundation for pose estimation, map alignment, and drift correction in visual SLAM~\cite{hartley2004multiple}.
